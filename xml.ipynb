{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43317b5a",
   "metadata": {},
   "source": [
    "## This is a script that will parse the all the data within the site map. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab3658ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FeatureNotFound",
     "evalue": "Couldn't find a tree builder with the features you requested: xml. Do you need to install a parser library?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFeatureNotFound\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m     xml_df\u001b[39m.\u001b[39mto_csv(\u001b[39m\"\u001b[39m\u001b[39mscr_xml_files.csv\u001b[39m\u001b[39m\"\u001b[39m,encoding \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mISO-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m     \u001b[39mreturn\u001b[39;00m xml_df\n\u001b[0;32m---> 30\u001b[0m readDocs_Xml()   \n",
      "Cell \u001b[0;32mIn[1], line 14\u001b[0m, in \u001b[0;36mreadDocs_Xml\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m sitemap \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttps://docs.netapp.com/us-en/nfs-plugin-vmware-vaai/sitemap.xml\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m cloudURL \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mget(sitemap)\n\u001b[0;32m---> 14\u001b[0m soup \u001b[39m=\u001b[39m BeautifulSoup(cloudURL\u001b[39m.\u001b[39;49mcontent, \u001b[39m\"\u001b[39;49m\u001b[39mxml\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     16\u001b[0m \u001b[39m# Within the sitemap, we see that all the URL's are within the \"loc\" tag. We want this information.\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39m# We want to return all the information within the tag by using the \"findAll\" function. \u001b[39;00m\n\u001b[1;32m     18\u001b[0m url \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/dataplus/lib/python3.11/site-packages/bs4/__init__.py:250\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m     builder_class \u001b[39m=\u001b[39m builder_registry\u001b[39m.\u001b[39mlookup(\u001b[39m*\u001b[39mfeatures)\n\u001b[1;32m    249\u001b[0m     \u001b[39mif\u001b[39;00m builder_class \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[39mraise\u001b[39;00m FeatureNotFound(\n\u001b[1;32m    251\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find a tree builder with the features you \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    252\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mrequested: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m. Do you need to install a parser library?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    253\u001b[0m             \u001b[39m%\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(features))\n\u001b[1;32m    255\u001b[0m \u001b[39m# At this point either we have a TreeBuilder instance in\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[39m# builder, or we have a builder_class that we can instantiate\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[39m# with the remaining **kwargs.\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[39mif\u001b[39;00m builder \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mFeatureNotFound\u001b[0m: Couldn't find a tree builder with the features you requested: xml. Do you need to install a parser library?"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from pathlib import Path \n",
    "import os\n",
    "\n",
    "# function that reads and returns the xml data from the sitemap. \n",
    "\n",
    "def readDocs_Xml():\n",
    "\n",
    "    # This requests the data from the URL\n",
    "    sitemap = \"https://docs.netapp.com/us-en/nfs-plugin-vmware-vaai/sitemap.xml\"\n",
    "    cloudURL = requests.get(sitemap)\n",
    "    soup = BeautifulSoup(cloudURL.content, \"xml\")\n",
    "    \n",
    "    # Within the sitemap, we see that all the URL's are within the \"loc\" tag. We want this information.\n",
    "    # We want to return all the information within the tag by using the \"findAll\" function. \n",
    "    url = []\n",
    "    url = soup.findAll('loc')\n",
    "    child = soup.get_text('loc').strip(\"loc\")\n",
    "    print(child)\n",
    "    \n",
    "        \n",
    "    #Create data frame\n",
    "    xml_df = pd.DataFrame(url)\n",
    "    xml_df.columns = ['URL']\n",
    "    xml_df.to_csv(\"scr_xml_files.csv\",encoding = \"ISO-8859-1\")\n",
    "    return xml_df\n",
    "    \n",
    "readDocs_Xml()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d198bf53",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#final_files_df = read_csv(\"/Users/aleca/Downloads/scrape_xml_files\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
